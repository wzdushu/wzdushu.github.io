<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>k8s on 丸子有记</title><link>/tags/k8s/</link><description>Recent content in k8s on 丸子有记</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>wzdushu@gmail.com (wanzi)</managingEditor><webMaster>wzdushu@gmail.com (wanzi)</webMaster><copyright>丸子有记</copyright><lastBuildDate>Mon, 19 Aug 2024 10:10:42 +0800</lastBuildDate><atom:link href="/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Summary of CKA Examination Experience in 2024</title><link>/post/kubernetes-cka-2024/</link><pubDate>Mon, 19 Aug 2024 10:10:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-cka-2024/</guid><description>I have wanted to take a cloud native &amp;amp; k8s related certificate for a long time. The sooner the better. I have been postponing it until this year due to work reasons. I have seen its price increase twice (it hurts a little bit to say this). Recently, I finally had time to spare a week to prepare for the exam. Since I have been using kubernetes in my work</description></item><item><title>Using Alibaba Cloud's open source solution to enable multiple people to use a single GPU</title><link>/post/kubernetes-aliyun-gpushare-deploy/</link><pubDate>Fri, 30 Jun 2023 18:10:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-aliyun-gpushare-deploy/</guid><description>Recently, a new AI project was launched, which mainly provides AI online experiments for universities. The project also purchased a GPU server, but there is only one Nvidia Tesla T4 card, which needs to support multiple students to do experiments online at the same time. The online experiment system is currently running on Kubernetes, so it is necessary to consider GPU sharing in the k8s environment. Alibaba Cloud GPU card</description></item><item><title>Kind builds a lightweight kubernetes cluster</title><link>/post/kubernetes-kind-build-clusters/</link><pubDate>Thu, 13 Apr 2023 18:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-kind-build-clusters/</guid><description>I was reviewing Golang recently and wrote a web application. After running it locally, I wanted to test it in a k8s cluster. Due to the machine configuration, it was still a bit difficult to build a complete k8s cluster. I remember that a friend said that k8s can also be run in docker, so I tried it. Today&amp;rsquo;s protagonist is kind, so what is kind? What can kind be</description></item><item><title>Jenkins Workspace and Maven cache local persistent storage</title><link>/post/cicd-jenkins-localstorage-workspace-maven/</link><pubDate>Wed, 22 Mar 2023 16:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-jenkins-localstorage-workspace-maven/</guid><description>Create local storage Since this is a test cluster, we can directly use local volume storage here; Since the user running in the jenkins server here is jenkins, and the uid of jenkins is 1000, we need to grant /opt/jenkins_agent/ and /opt/jenkins_maven/ permissions to jenkins in advance on node1 Copy 1 2 chown 1000.1000 -R /opt/jenkins_agent -R chown 1000.1000 -R /opt/jenkins_maven/ -R Local storage: agent-pv-pvc.yaml Copy 1 2 3 4</description></item><item><title>Deploy Jenkins in kubernetes environment</title><link>/post/cicd-jenkins-in-kubernetes/</link><pubDate>Thu, 16 Mar 2023 16:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-jenkins-in-kubernetes/</guid><description>1. Install and deploy Jenkins 1. Manual installation Manual installation is very simple. Just prepare the yaml configuration in advance. The content of all CICD resources jenkins-install.yaml is as follows: Copy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: v1 kind: PersistentVolume metadata: name: jenkins-pv spec: storageClassName: local</description></item><item><title>Solve nginx file upload restrictions and 504 gateway timeout under k8s</title><link>/post/kubernetes-nginx-ingress-504/</link><pubDate>Mon, 08 Nov 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-nginx-ingress-504/</guid><description>Recently, there are two problems that need to be solved when using k8s clusters for business. Here are some records: 1M limit for uploading files on the front-end page 504 timeout occurs when the front-end page sends a POST request to the back-end The main solution to the first problem: the default upload size of nginx is 1M, and the following content is added to the http, server, location area</description></item><item><title>阿里云ACK同时支持公网和内网SLB</title><link>/post/kubernetes-aliyun-ingress-slb-intranet/</link><pubDate>Thu, 21 Oct 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-aliyun-ingress-slb-intranet/</guid><description>一、背景 有一套ACK集群 成功部署Nginx ingress controller并绑定了公网SLB 注意：通过阿里云容器服务管理控制台创建的Kubernete</description></item><item><title>利用Prometheusalert打造Prometheus告警消息聚合</title><link>/post/prometheus-prometheusalert-alertmanager/</link><pubDate>Sat, 18 Sep 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/prometheus-prometheusalert-alertmanager/</guid><description>部署prometheusalert Copy 1 2 3 4 git clone https://github.com/feiyu563/PrometheusAlert.git cd PrometheusAlert/example/helm/prometheusalert #更新config/app.conf设置登陆用户信息，并配置数据库信息 helm install -n monitoring . 创建企</description></item><item><title>制定kubernetes集群备份策略</title><link>/post/kubernetes-velero-etcd-backup/</link><pubDate>Sat, 11 Sep 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-velero-etcd-backup/</guid><description>对于备份，每家互联网公司技术人员都要去做的一件事儿，当然我们也不例外，今天我主要针对生产环境kubernetes集群制定一些自己的策略，这里</description></item><item><title>kubeadm部署高可用kubernetes集群</title><link>/post/kubernetes-kubeadm-haproxy-keepalived-deploy/</link><pubDate>Sun, 15 Aug 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-kubeadm-haproxy-keepalived-deploy/</guid><description>为了后便后期验证私有化部署 ，最近内网环境需要快速搭建一套k8s集群，由于之前对于规模比较大的集群，我一般采用kubeasz和kubespra</description></item><item><title>ArgoCD配合Jenkins Pipeline自动化部署应用</title><link>/post/cicd-argocd-jenkins-pipeline/</link><pubDate>Wed, 29 Jul 2020 15:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-argocd-jenkins-pipeline/</guid><description>创建helm仓库 首先，创建基础Helm模版仓库： Copy 1 helm create template . 对于实际的部署中，需要根据自己的业务定制自己的helm模版，我这里直接使用我们内</description></item><item><title>argocd部署deployment出现: no space left on device</title><link>/post/kubernetes-error-no-space-left-on-device/</link><pubDate>Mon, 18 May 2020 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-error-no-space-left-on-device/</guid><description>故障现象 上午通过argocd部署几个业务应用，部署了2个以后，第三方死活部署不成功，相同的配置，知识集群不一样，怎么会出现这样的问题呢？ 于是</description></item><item><title>ArgoCD添加多集群</title><link>/post/cicd-argocd-add-clusters/</link><pubDate>Tue, 05 May 2020 15:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-argocd-add-clusters/</guid><description>生成argocd管理用户token 登陆dashboard，settings&amp;ndash;&amp;gt;Accounts&amp;ndash;&amp;gt;adm</description></item><item><title>ArgoCD安部部署</title><link>/post/cicd-argocd-install-in-k8s/</link><pubDate>Fri, 01 May 2020 15:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-argocd-install-in-k8s/</guid><description>安装部署 ArgoCD的部署非常简单，安装官方的部署方法(HA模式： Copy 1 2 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v1.5.2/manifests/ha/install.yaml 可以按照需求调整部署文件，待pod顺利启动后 Copy 1</description></item><item><title>kubernetes集群添加用户</title><link>/post/kubernetes-add-user/</link><pubDate>Tue, 31 Dec 2019 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-add-user/</guid><description>之前通过ansible搭建了kubernetes集群环境,这里需求主要是添加一个用户进行日常管理，并限制到指定的namespace，接下来进</description></item><item><title>kubernetes集群部署traefik2.1</title><link>/post/kubernetes-traefik-v2/</link><pubDate>Tue, 17 Dec 2019 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-traefik-v2/</guid><description>架构&amp;amp;概念 Traefik2.x版本相比1.7.x架构有很大变化，正如上边这张架构图，最主要的功能是支持了TCP协议、增加了Route</description></item><item><title>kubeasz部署k8s集群</title><link>/post/kubernetes-kubeasz-deploy-automation/</link><pubDate>Thu, 12 Dec 2019 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-kubeasz-deploy-automation/</guid><description>环境准备 Master节点 Copy 1 2 3 172.16.244.14 172.16.244.16 172.16.244.18 Node节点 Copy 1 2 172.16.244.25 172.16.244.27 Master节点VIP地址: 172.16.243.13 部署工具:Ansible/kubeasz 初始化环境</description></item><item><title>基于K8S部署gitlab-runner</title><link>/post/cicd-gitlab-k8s-gitlabrunner/</link><pubDate>Thu, 14 Nov 2019 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-gitlab-k8s-gitlabrunner/</guid><description>部署gitlab-runner 这里基于helm部署，参考：https://gitlab.com/gitlab-org/charts/gitl</description></item></channel></rss>