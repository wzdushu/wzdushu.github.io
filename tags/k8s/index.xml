<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>k8s on 丸子有记</title><link>/tags/k8s/</link><description>Recent content in k8s on 丸子有记</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>wzdushu@gmail.com (wanzi)</managingEditor><webMaster>wzdushu@gmail.com (wanzi)</webMaster><copyright>丸子有记</copyright><lastBuildDate>Mon, 19 Aug 2024 10:10:42 +0800</lastBuildDate><atom:link href="/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Summary of CKA Examination Experience in 2024</title><link>/post/kubernetes-cka-2024/</link><pubDate>Mon, 19 Aug 2024 10:10:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-cka-2024/</guid><description>I have wanted to take a cloud native &amp;amp; k8s related certificate for a long time. The sooner the better. I have been postponing it until this year due to work reasons. I have seen its price increase twice (it hurts a little bit to say this). Recently, I finally had time to spare a week to prepare for the exam. Since I have been using kubernetes in my work</description></item><item><title>Using Alibaba Cloud's open source solution to enable multiple people to use a single GPU</title><link>/post/kubernetes-aliyun-gpushare-deploy/</link><pubDate>Fri, 30 Jun 2023 18:10:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-aliyun-gpushare-deploy/</guid><description>Recently, a new AI project was launched, which mainly provides AI online experiments for universities. The project also purchased a GPU server, but there is only one Nvidia Tesla T4 card, which needs to support multiple students to do experiments online at the same time. The online experiment system is currently running on Kubernetes, so it is necessary to consider GPU sharing in the k8s environment. Alibaba Cloud GPU card</description></item><item><title>Kind builds a lightweight kubernetes cluster</title><link>/post/kubernetes-kind-build-clusters/</link><pubDate>Thu, 13 Apr 2023 18:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-kind-build-clusters/</guid><description>I was reviewing Golang recently and wrote a web application. After running it locally, I wanted to test it in a k8s cluster. Due to the machine configuration, it was still a bit difficult to build a complete k8s cluster. I remember that a friend said that k8s can also be run in docker, so I tried it. Today&amp;rsquo;s protagonist is kind, so what is kind? What can kind be</description></item><item><title>Jenkins Workspace and Maven cache local persistent storage</title><link>/post/cicd-jenkins-localstorage-workspace-maven/</link><pubDate>Wed, 22 Mar 2023 16:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-jenkins-localstorage-workspace-maven/</guid><description>Create local storage Since this is a test cluster, we can directly use local volume storage here; Since the user running in the jenkins server here is jenkins, and the uid of jenkins is 1000, we need to grant /opt/jenkins_agent/ and /opt/jenkins_maven/ permissions to jenkins in advance on node1 Copy 1 2 chown 1000.1000 -R /opt/jenkins_agent -R chown 1000.1000 -R /opt/jenkins_maven/ -R Local storage: agent-pv-pvc.yaml Copy 1 2 3 4</description></item><item><title>Deploy Jenkins in kubernetes environment</title><link>/post/cicd-jenkins-in-kubernetes/</link><pubDate>Thu, 16 Mar 2023 16:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-jenkins-in-kubernetes/</guid><description>1. Install and deploy Jenkins 1. Manual installation Manual installation is very simple. Just prepare the yaml configuration in advance. The content of all CICD resources jenkins-install.yaml is as follows: Copy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: v1 kind: PersistentVolume metadata: name: jenkins-pv spec: storageClassName: local</description></item><item><title>Solve nginx file upload restrictions and 504 gateway timeout under k8s</title><link>/post/kubernetes-nginx-ingress-504/</link><pubDate>Mon, 08 Nov 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-nginx-ingress-504/</guid><description>Recently, there are two problems that need to be solved when using k8s clusters for business. Here are some records: 1M limit for uploading files on the front-end page 504 timeout occurs when the front-end page sends a POST request to the back-end The main solution to the first problem: the default upload size of nginx is 1M, and the following content is added to the http, server, location area</description></item><item><title>Alibaba Cloud ACK supports both public and private SLB</title><link>/post/kubernetes-aliyun-ingress-slb-intranet/</link><pubDate>Thu, 21 Oct 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-aliyun-ingress-slb-intranet/</guid><description>1. Background There is an ACK cluster Successfully deployed Nginx ingress controller and bound to the public network SLB Note: The Kubernetes cluster created through the Alibaba Cloud Container Service Management Console will automatically deploy a set of Nginx Ingress Controllers during initialization, which is mounted on the public network SLB instance by default. 2. Configuration 1. Create an intranet LB Alibaba Cloud Console, create an intranet SLB and bind</description></item><item><title>Using Prometheusalert to build Prometheus alert message aggregation</title><link>/post/prometheus-prometheusalert-alertmanager/</link><pubDate>Sat, 18 Sep 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/prometheus-prometheusalert-alertmanager/</guid><description>Deploy prometheusalert Copy 1 2 3 4 git clone https://github.com/feiyu563/PrometheusAlert.git cd PrometheusAlert/example/helm/prometheusalert #Update config/app.conf to set login user information and configure database information helm install -n monitoring . Create an enterprise WeChat group robot After the enterprise WeChat group, click the enterprise WeChat group, right-click &amp;ldquo;Add group robot&amp;rdquo;, and create a group robot to get a robot webhook address, record the address for backup. Prometheus access configuration Configure alertmanager Since</description></item><item><title>Use Velero to configure kubernetes cluster backup strategy</title><link>/post/kubernetes-velero-etcd-backup/</link><pubDate>Sat, 11 Sep 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-velero-etcd-backup/</guid><description>Backup is something that every Internet company technician has to do, and of course we are no exception. Today I mainly formulate some of my own strategies for the production environment kubernetes cluster, and share them with you here. The purpose of my kubernetes backup here is mainly to prevent the following situations: Prevent accidental deletion of a namespace in the cluster Prevent accidental operation from causing an abnormality in</description></item><item><title>kubeadm deploys a highly available kubernetes cluster</title><link>/post/kubernetes-kubeadm-haproxy-keepalived-deploy/</link><pubDate>Sun, 15 Aug 2021 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-kubeadm-haproxy-keepalived-deploy/</guid><description>In order to verify the private deployment later , the intranet environment needs to quickly build a k8s cluster. Because I usually use kubeasz and kubespray to handle large-scale clusters before, this time for small environment clusters, it will be more efficient to directly use kubeadm to deploy. The following records the kubeadm deployment process: Cluster nodes: Copy 1 2 3 4 192.168.1.206 sd-cluster-206 node 192.168.1.207 sd-cluster-207 master,etcd 192.168.1.208 sd-cluster-208</description></item><item><title>ArgoCD+Jenkins Pipeline Automated Application Deployment</title><link>/post/cicd-argocd-jenkins-pipeline/</link><pubDate>Wed, 29 Jul 2020 15:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-argocd-jenkins-pipeline/</guid><description>Create helm repository First, create a basic Helm template repository: Copy 1 helm create template . For actual deployment, you need to customize your own helm template according to your business. I will directly use our internal custom universal template for quick deployment; you can also refer to the helm chart maintained by bitnami (https://github.com/bitnami/charts/tree/master/bitnami) Jenkins credential configuration argocd token information Configure Jenkins pipeline writing Here we take the gotest</description></item><item><title>Argocd deployment appears: no space left on device</title><link>/post/kubernetes-error-no-space-left-on-device/</link><pubDate>Mon, 18 May 2020 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-error-no-space-left-on-device/</guid><description>Failure phenomenon In the morning, several business applications were deployed through argocd. After deploying 2, the third-party deployment failed. The same configuration, different knowledge clusters, how could such a problem occur? So I checked the log, as follows: Copy 1 2 3 Warning Failed 1m kubelet, 172.16.25.13 Error: Error response from daemon: error creating overlay mount to /var/lib/docker/overlay2/ba37165607862efb350093e5e287207e2547759fd81dc4e5e356a86ac5e28324-init/merged: no space left on device Warning Failed 1m kubelet, 172.16.25.13 Error: Error</description></item><item><title>ArgoCD adds multi-cluster</title><link>/post/cicd-argocd-add-clusters/</link><pubDate>Tue, 05 May 2020 15:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-argocd-add-clusters/</guid><description>Generate argocd management user token Log in to the dashboard, settings&amp;ndash;&amp;gt;Accounts&amp;ndash;&amp;gt;admin&amp;ndash;&amp;gt;Generate New After generating, please record the token information, similar to the following: Copy 1 fyJhbGciOiJ3UzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiI2OWI0M2M0Mi01MmZiLTRlZmItODIxOC0yOWU3NGM5MWI0NDIiLCJpYXQiOjE1OTUzMTEx3zQsImlzcyI6ImFyZ29jZCIsIm5iZiI6MTU5NTMxMTE3NCwic3ViIjoib3duZXIifQ.9u4XzArEeaz7G2Q2TWusnTkakEmq9BYDAUHr3dC6wG5 Configure argocd config For argocd with https authentication enabled, it is useless when adding a cluster. You need to log in to the server-side POD for configuration, as follows: Copy 1 2 3 4 5 6 7 8 9 10 11 12 13 #</description></item><item><title>ArgoCD installation and deployment</title><link>/post/cicd-argocd-install-in-k8s/</link><pubDate>Fri, 01 May 2020 15:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-argocd-install-in-k8s/</guid><description>Installation and deployment ArgoCD is very easy to deploy. Install the official deployment method (HA mode: Copy 1 2 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v1.5.2/manifests/ha/install.yaml You can adjust the deployment file as needed. After the pod is successfully started Copy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # kubectl -n</description></item><item><title>Adding users to a kubernetes cluster</title><link>/post/kubernetes-add-user/</link><pubDate>Tue, 31 Dec 2019 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-add-user/</guid><description>Previously, we built a kubernetes cluster environment through ansible. The main requirement here is to add a user for daily management and limit it to the specified namespace. The following operation is performed: Users in kubernetes There are two types of users (User) in K8S - service accounts (ServiceAccount) and users in the ordinary sense (User). ServiceAccount is managed by K8S, while User is usually managed externally. K8S does not</description></item><item><title>Deploy traefik2.1 in kubernetes cluster</title><link>/post/kubernetes-traefik-v2/</link><pubDate>Tue, 17 Dec 2019 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-traefik-v2/</guid><description>Architecture &amp;amp; Concepts Traefik 2.x has a big change compared to 1.7.x architecture. As shown in the architecture diagram above, the main function is to support TCP protocol and add the concept of Router. Here we use Traefik 2.1 deployed in the kubernetes cluster. Business access is requested to traefik Ingress through haproxy. The following are some concepts involved in the construction process: EntryPoints: Traefik&amp;rsquo;s network entry, defining the port</description></item><item><title>kubeasz deploys k8s cluster</title><link>/post/kubernetes-kubeasz-deploy-automation/</link><pubDate>Thu, 12 Dec 2019 10:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/kubernetes-kubeasz-deploy-automation/</guid><description>Environment preparation Master node Copy 1 2 3 172.16.244.14 172.16.244.16 172.16.244.18 Node node Copy 1 2 172.16.244.25 172.16.244.27 Master node VIP address: 172.16.243.13 Deployment tool: Ansible/kubeasz Initialize the environment Install Ansible Copy 1 2 3 4 5 apt update apt-get install ansible expect git clone https://github.com/easzlab/kubeasz cd kubeasz cp * /etc/ansible/ Configure ansible for password-free login Copy 1 2 ssh-keygen -t rsa -b 2048 #Generate key ./tools/yc-ssh-key-copy.sh hosts root &amp;#39;rootpassword&amp;#39;</description></item><item><title>Deploy gitlab-runner based on K8S</title><link>/post/cicd-gitlab-k8s-gitlabrunner/</link><pubDate>Thu, 14 Nov 2019 17:22:42 +0800</pubDate><author>wzdushu@gmail.com (wanzi)</author><guid>/post/cicd-gitlab-k8s-gitlabrunner/</guid><description>Deploy gitlab-runner Here is based on helm deployment, reference: https://gitlab.com/gitlab-org/charts/gitlab-runner.git Copy 1 helm install --namespace gitlab-managed-apps --name k8s-gitlab-runner -f values.yaml Note: the values.yaml file needs to be set to privileged: true Build a basic image (docker in docker) Dockerfile file content: Copy 1 2 3 4 5 6 7 FROM docker:19.03.1-dind WORKDIR /opt RUN echo &amp;#34;nameserver 114.114.114.114&amp;#34; &amp;gt;&amp;gt; /etc/resolv.conf RUN sed -i &amp;#39;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&amp;#39; /etc/apk/repositories RUN apk update RUN apk upgrade</description></item></channel></rss>